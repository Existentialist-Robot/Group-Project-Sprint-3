{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_prep_cactus_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7kH42hTsmQh",
        "colab_type": "text"
      },
      "source": [
        "# Data Pre-Processing and Autoencoders for Dimensionality Reduction (Kalia)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rnsCH_Ksumn",
        "colab_type": "text"
      },
      "source": [
        "## Downloading the Data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSD0DYmJQPi5",
        "colab_type": "code",
        "outputId": "d9ecff88-0f65-4f8c-f8b4-e1168b8f70c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# connecting notebook to google drive\n",
        "# click on the URL, give permissions and copy and paste the authorisation code to connect\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "# access the relevant folder in the Google Drive to download the data in\n",
        "import os\n",
        "os.chdir('/gdrive/My Drive/Cactus')\n",
        "\n",
        "# downloading the data from Kaggle\n",
        "# make sure you have the right Kaggle library installed\n",
        "!pip3 install kaggle==1.5.6\n",
        "!kaggle -v\n",
        "\n",
        "# make a .kaggle folder in your drive \n",
        "!mkdir .kaggle\n",
        "# save kaggle.json file with your username and unique key in .kaggle\n",
        "!echo '{\"username\":\"USERNAME\",\"key\":\"UNIQUE_KEY\"}' > /root/.kaggle/kaggle.json\n",
        "# change the file permissions so you can read and write from this file (but not execute)\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "# get data from https://www.kaggle.com/c/aerial-cactus-identification/\n",
        "!kaggle competitions download -c aerial-cactus-identification\n",
        "\n",
        "# unzip data\n",
        "!unzip train.zip\n",
        "!unzip test.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOAmNP7bs2ei",
        "colab_type": "text"
      },
      "source": [
        "## Importing Libraries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFPxeMYk2Ogx",
        "colab_type": "code",
        "outputId": "5871456e-3711-4414-9a54-ff19a2653190",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import glob\n",
        "import imageio\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "import keras\n",
        "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
        "from keras.models import Model, load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUe3thUIszok",
        "colab_type": "text"
      },
      "source": [
        "## Data Pre-Processing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gK2JqG7z2v0H",
        "colab_type": "code",
        "outputId": "f5ea539e-6fb3-4711-b5b5-d4d0dc53f4e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "# importing the training label data\n",
        "train_label = pd.read_csv('train.csv')\n",
        "\n",
        "# viewing the first 5 lines of the data\n",
        "train_label.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqKARA8HodeS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# accessing the folder with the training data\n",
        "os.chdir('/gdrive/My Drive/Cactus/train')\n",
        "# list of filenames in the order images will be imported\n",
        "filenames = glob.glob('*.*')\n",
        "\n",
        "# list of dummy data the length of our filenames for new sorted labels\n",
        "sorted_cactus = [\"a\"]*len(filenames)\n",
        "\n",
        "# loop to search for the id in the filenames and train_label\n",
        "# and assign the right classification in the right order to the new list\n",
        "for i in range(len(filenames)):\n",
        "  for j in range(len(train_label)):\n",
        "    if filenames[i] == train_label[\"id\"][j]:\n",
        "      sorted_cactus[i] = train_label[\"has_cactus\"][j]\n",
        "\n",
        "# create new dataframe of sorted labels\n",
        "d = {'id': filenames, 'has_cactus': sorted_cactus}\n",
        "labels_sort = pd.DataFrame(data=d)\n",
        "\n",
        "# accessing the main folder to save the csv\n",
        "os.chdir('/gdrive/My Drive/Cactus')\n",
        "# save dataframe as csv to folder\n",
        "labels_sort.to_csv(\"train_label_sorted.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCgEw6DJ7MpM",
        "colab_type": "code",
        "outputId": "f0ee29af-80b3-450e-ad55-d93a0e2a6058",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "labels_sort.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpavMqf02zWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# standardised image size\n",
        "IMAGE_SIZE = (32, 32)\n",
        "\n",
        "# list we will save our image arrays to\n",
        "train_img = []\n",
        "\n",
        "# loop to resize and reshape the RGB images into arrays\n",
        "for index, filename in enumerate(glob.glob('train/*.*')):\n",
        "  # read the image files\n",
        "  image = imageio.imread(filename)\n",
        "  # resize the data for standardisation\n",
        "  image = cv2.resize(image, IMAGE_SIZE)\n",
        "  # convert the image data to an array\n",
        "  image = np.array(image)\n",
        "  # append the image array to our list\n",
        "  train_img.append(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBv5oINf3xXd",
        "colab_type": "code",
        "outputId": "5cb49c6f-eebe-4a5f-ee66-913fe7b72d63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(train_img))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFaoAQFh3v-P",
        "colab_type": "code",
        "outputId": "d6f50521-498f-4cae-f3a9-be19645fde2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "plt.imshow(train_img[150])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjDfGnK5Z4t5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating an array of our image data\n",
        "train_data = np.array(train_img)\n",
        "\n",
        "# standardising our image data\n",
        "train_data = train_data.astype('float32') / 255.\n",
        "\n",
        "# ensuring our images are the right shape\n",
        "train_data = train_data.reshape(len(train_data), 32, 32, 3)\n",
        "\n",
        "# splitting our data into a training, validation, and testing set\n",
        "train_data, eval_data, train_label, eval_label = train_test_split(train_data, labels_sort, random_state=42)\n",
        "eval_data, test_data, eval_label, test_label = train_test_split(eval_data, eval_label, test_size=0.5, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P4ENImuauVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# saving our original dataset and training, validation, and testing numpy arrays\n",
        "np.save(\"train_img\", train_img)\n",
        "np.save(\"train_data\", train_data)\n",
        "np.save(\"eval_data\", eval_data)\n",
        "np.save(\"test_data\", test_data)\n",
        "\n",
        "np.save(\"train_label\", train_label)\n",
        "np.save(\"eval_label\", eval_label)\n",
        "np.save(\"test_label\", test_label)\n",
        "\n",
        "# loading all our arrays\n",
        "train_img = np.load(\"train_img.npy\")\n",
        "train_data = np.load(\"train_data.npy\")\n",
        "eval_data = np.load(\"eval_data.npy\")\n",
        "test_data = np.load(\"test_data.npy\")\n",
        "\n",
        "train_label = np.load(\"train_label.npy\")\n",
        "eval_label = np.load(\"eval_label.npy\")\n",
        "test_label = np.load(\"test_label.npy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZwkT6mks-6R",
        "colab_type": "text"
      },
      "source": [
        "## Autoencoders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SsGKkeLtB4x",
        "colab_type": "text"
      },
      "source": [
        "### Autoencoder 1 (8 layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGVpqera4OCl",
        "colab_type": "code",
        "outputId": "5bf9c785-e6be-432b-b21c-819cb918f477",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "# this is our input placeholder\n",
        "input_img = Input(shape=(32, 32, 3))  \n",
        "\n",
        "## ENCODER ##\n",
        "# Conv1 #\n",
        "encoded = Conv2D(filters = 256, kernel_size = (3, 3), activation='relu', padding='same')(input_img)\n",
        "encoded = Conv2D(filters = 256, kernel_size = (3, 3), activation='relu', padding='same')(encoded)\n",
        "encoded = MaxPooling2D(pool_size = (2, 2), padding='same', strides=2)(encoded)\n",
        "print(encoded.shape)\n",
        "\n",
        "# Conv2 #\n",
        "encoded = Conv2D(filters = 128, kernel_size = (3, 3), activation='relu', padding='same')(encoded)\n",
        "encoded = Conv2D(filters = 128, kernel_size = (3, 3), activation='relu', padding='same')(encoded)\n",
        "encoded = MaxPooling2D(pool_size = (2, 2), padding='same', strides=2)(encoded)\n",
        "print(encoded.shape)\n",
        "\n",
        "# Conv3 #\n",
        "encoded = Conv2D(filters = 64, kernel_size = (3, 3), activation='relu', padding='same')(encoded)\n",
        "encoded = Conv2D(filters = 64, kernel_size = (3, 3), activation='relu', padding='same')(encoded)\n",
        "encoded = MaxPooling2D(pool_size = (2, 2), padding='same', strides=2)(encoded)\n",
        "print(encoded.shape)\n",
        "\n",
        "# Conv4 #\n",
        "encoded = Conv2D(filters = 32, kernel_size = (3, 3), activation='relu', padding='same')(encoded)\n",
        "encoded = Conv2D(filters = 32, kernel_size = (3, 3), activation='relu', padding='same')(encoded)\n",
        "encoded = MaxPooling2D(pool_size = (2, 2), padding='same', strides=2)(encoded)\n",
        "print(encoded.shape)\n",
        "\n",
        "\n",
        "## DECODER ##\n",
        "\n",
        "# DeConv1\n",
        "decoded = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
        "decoded = Conv2D(32, (3, 3), activation='relu', padding='same')(decoded)\n",
        "decoded = UpSampling2D((2, 2))(decoded)\n",
        "print(decoded.shape)\n",
        "\n",
        "# DeConv2\n",
        "decoded = Conv2D(64, (3, 3), activation='relu', padding='same')(decoded)\n",
        "decoded = Conv2D(64, (3, 3), activation='relu', padding='same')(decoded)\n",
        "decoded = UpSampling2D((2, 2))(decoded)\n",
        "print(decoded.shape)\n",
        "\n",
        "# DeConv3\n",
        "decoded = Conv2D(128, (3, 3), activation='relu', padding='same')(decoded)\n",
        "decoded = Conv2D(128, (3, 3), activation='relu', padding='same')(decoded)\n",
        "decoded = UpSampling2D((2, 2))(decoded)\n",
        "print(decoded.shape)\n",
        "\n",
        "# DeConv4\n",
        "decoded = Conv2D(256, (3, 3), activation='relu', padding='same')(decoded)\n",
        "decoded = Conv2D(256, (3, 3), activation='relu', padding='same')(decoded)\n",
        "decoded = UpSampling2D((2, 2))(decoded)\n",
        "print(decoded.shape)\n",
        "\n",
        "decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(decoded)\n",
        "print(decoded.shape)\n",
        "\n",
        "\n",
        "## ENCODER and AUTOENCODER ##\n",
        "\n",
        "autoencoder = Model(input_img, decoded)\n",
        "encoder = Model(input_img, encoded)\n",
        "\n",
        "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVlMLNnp5xcq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fitting the training data to the autoencoder model\n",
        " autoencoder.fit(train_data, train_data,\n",
        "                 epochs=1000,\n",
        "                 batch_size=256,\n",
        "                 shuffle=True,\n",
        "                 validation_data=(eval_data, eval_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff4Zsw2Rrom7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# saving whole model\n",
        "autoencoder.save('autoencoders/autoencoder_model1.h5')\n",
        "\n",
        "# saving encoder for dimensionality reduction\n",
        "encoder.save('autoencoders/encoder_model1.h5')\n",
        " \n",
        "# loading whole model\n",
        "model1 = load_model('autoencoders/autoencoder_model1.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkdeqlR7nRPX",
        "colab_type": "code",
        "outputId": "355fb548-416e-43f7-964c-f481a6f72c9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "# history.history stores the training and validation loss of our model when fitting\n",
        "plt.plot(autoencoder.history.history['loss'])\n",
        "plt.plot(autoencoder.history.history['val_loss'])\n",
        "plt.title('Autoencoder loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkEC74XHfqE4",
        "colab_type": "code",
        "outputId": "dd701814-490c-441d-826c-f0093783d4d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "# reconstructed images from validation set\n",
        "reconst_test = autoencoder.predict(eval_data)\n",
        "\n",
        "# number of images\n",
        "n = 10\n",
        "# number of rows in plot\n",
        "row = 2\n",
        "\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # display original\n",
        "    ax = plt.subplot(row, n, i + 1)\n",
        "    plt.imshow(eval_data[i].reshape(32, 32, 3))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruction\n",
        "    ax = plt.subplot(row, n, i + 1 + n)\n",
        "    plt.imshow(reconst_test[i].reshape(32, 32, 3))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzjtsM2ftIp9",
        "colab_type": "text"
      },
      "source": [
        "### Autoencoder 2 (6 layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb8SZSaw8GqL",
        "colab_type": "code",
        "outputId": "3e0cb446-fa3f-4d6e-9175-b31810178b20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "# input layer\n",
        "input_img_2 = Input(shape=(32, 32, 3))  \n",
        "\n",
        "## ENCODER ##\n",
        "# Conv1 #\n",
        "encoded_2 = Conv2D(filters = 256, kernel_size = (3, 3), activation='relu', padding='same')(input_img_2)\n",
        "encoded_2 = Conv2D(filters = 256, kernel_size = (3, 3), activation='relu', padding='same')(encoded_2)\n",
        "encoded_2 = MaxPooling2D(pool_size = (2, 2), padding='same', strides=2)(encoded_2)\n",
        "print(encoded_2.shape)\n",
        "\n",
        "# Conv2 #\n",
        "encoded_2 = Conv2D(filters = 128, kernel_size = (3, 3), activation='relu', padding='same')(encoded_2)\n",
        "encoded_2 = Conv2D(filters = 128, kernel_size = (3, 3), activation='relu', padding='same')(encoded_2)\n",
        "encoded_2 = MaxPooling2D(pool_size = (2, 2), padding='same', strides=2)(encoded_2)\n",
        "print(encoded_2.shape)\n",
        "\n",
        "# Conv3 #\n",
        "encoded_2 = Conv2D(filters = 64, kernel_size = (3, 3), activation='relu', padding='same')(encoded_2)\n",
        "encoded_2 = Conv2D(filters = 64, kernel_size = (3, 3), activation='relu', padding='same')(encoded_2)\n",
        "encoded_2 = MaxPooling2D(pool_size = (2, 2), padding='same', strides=2)(encoded_2)\n",
        "print(encoded_2.shape)\n",
        "\n",
        "## DECODER ##\n",
        "\n",
        "# DeConv1\n",
        "decoded_2 = Conv2D(64, (3, 3), activation='relu', padding='same')(encoded_2)\n",
        "decoded_2 = Conv2D(64, (3, 3), activation='relu', padding='same')(decoded_2)\n",
        "decoded_2 = UpSampling2D((2, 2))(decoded_2)\n",
        "print(decoded_2.shape)\n",
        "\n",
        "# DeConv2\n",
        "decoded_2 = Conv2D(128, (3, 3), activation='relu', padding='same')(decoded_2)\n",
        "decoded_2 = Conv2D(128, (3, 3), activation='relu', padding='same')(decoded_2)\n",
        "decoded_2 = UpSampling2D((2, 2))(decoded_2)\n",
        "print(decoded_2.shape)\n",
        "\n",
        "# DeConv3\n",
        "decoded_2 = Conv2D(256, (3, 3), activation='relu', padding='same')(decoded_2)\n",
        "decoded_2 = Conv2D(256, (3, 3), activation='relu', padding='same')(decoded_2)\n",
        "decoded_2 = UpSampling2D((2, 2))(decoded_2)\n",
        "print(decoded_2.shape)\n",
        "\n",
        "decoded_2 = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(decoded_2)\n",
        "print(decoded_2.shape)\n",
        "\n",
        "\n",
        "## ENCODER and AUTOENCODER ##\n",
        "\n",
        "autoencoder_2 = Model(input_img_2, decoded_2)\n",
        "encoder_2 = Model(input_img_2, encoded_2)\n",
        "\n",
        "autoencoder_2.compile(optimizer='adadelta', loss='binary_crossentropy')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHE8pFbq9WkU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fitting the training data to the autoencoder model\n",
        " autoencoder_2.fit(train_data, train_data,\n",
        "                 epochs=1000,\n",
        "                 batch_size=256,\n",
        "                 shuffle=True,\n",
        "                 validation_data=(eval_data, eval_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2johqXyrkSz",
        "colab_type": "code",
        "outputId": "d51ba451-74a7-4185-80d1-217cec8e340e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "# history.history stores the training and validation loss of our model when fitting\n",
        "plt.plot(autoencoder_2.history.history['loss'])\n",
        "plt.plot(autoencoder_2.history.history['val_loss'])\n",
        "plt.title('Autoencoder loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMwd5lgS9c42",
        "colab_type": "code",
        "outputId": "81769fdd-c7e1-4d49-bdee-c67d428a65a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "# reconstructed images from validation set\n",
        "reconst_test_2 = autoencoder_2.predict(eval_data)\n",
        "\n",
        "# number of images\n",
        "n = 10\n",
        "# number of rows in plot\n",
        "row = 2\n",
        "\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # display original\n",
        "    ax = plt.subplot(row, n, i + 1)\n",
        "    plt.imshow(eval_data[i].reshape(32, 32, 3))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruction\n",
        "    ax = plt.subplot(row, n, i + 1 + n)\n",
        "    plt.imshow(reconst_test_2[i].reshape(32, 32, 3))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ov_lx5NTrtot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "autoencoder_2.save('autoencoders/autoencoder_model2.h5')\n",
        "encoder_2.save('autoencoders/encoder_model2.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuYPyTRTtPKK",
        "colab_type": "text"
      },
      "source": [
        "# Principle Component Analysis (PCA) (Sandie)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1IyD4FMr_87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPPXVphRwJuV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca_x = np.reshape(train_data, (24000,96))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pM8PSA5wKNJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(pca_x)\n",
        "scaled_x = scaler.transform(pca_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlAXmtWUwMH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(0.95)\n",
        "\n",
        "pca_transformed = pca.fit_transform(scaled_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pca_transformed.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inv = pca.inverse_transform(pca_transformed)\n",
        "\n",
        "plt.scatter(scaled_x[:, 0], scaled_x[:, 1], alpha=0.2)\n",
        "plt.scatter(inv[:, 0], inv[:, 1], alpha=0.8)\n",
        "plt.axis('equal');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2,5,figsize=(20,4),\n",
        " subplot_kw={'xticks':[], 'yticks':[]},\n",
        " gridspec_kw=dict(hspace=0.01, wspace=0.01))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(pca.components_[i].reshape(12,8), cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pca.explained_variance_ratio_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pca2 = PCA(10)\n",
        "\n",
        "pca_transformed2 = pca2.fit_transform(scaled_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inv2 = pca2.inverse_transform(pca_transformed2)\n",
        "\n",
        "plt.scatter(scaled_x[:, 0], scaled_x[:, 1], alpha=0.2)\n",
        "plt.scatter(inv2[:, 0], inv2[:, 1], alpha=0.8)\n",
        "plt.axis('equal');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2,5,figsize=(20,4),\n",
        " subplot_kw={'xticks':[], 'yticks':[]},\n",
        " gridspec_kw=dict(hspace=0.01, wspace=0.01))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(pca2.components_[i].reshape(12,8), cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pca2.explained_variance_ratio_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pca = PCA().fit(scaled_x)\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('number of components')\n",
        "plt.ylabel('cumulative explained variance');"
      ]
    }
  ]
}